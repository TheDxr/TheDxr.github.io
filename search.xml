<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>机器学习常用线性代数知识</title>
    <url>/2020/03/DLmath/</url>
    <content><![CDATA[<h1 id="线性代数基础"><a href="#线性代数基础" class="headerlink" title="线性代数基础"></a>线性代数基础</h1><h2 id="线性代数本质"><a href="#线性代数本质" class="headerlink" title="线性代数本质"></a>线性代数本质</h2><a id="more"></a>
<iframe src="//player.bilibili.com/player.html?aid=6043439&bvid=BV1ns41167b9&cid=9809358&page=1" width="800" height="600" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<h2 id="特性值和特征向量"><a href="#特性值和特征向量" class="headerlink" title="特性值和特征向量"></a>特性值和特征向量</h2><p>设A是n阶方阵，如果数λ和n维非零列向量x使关系式Ax=λx成立，那么这样的数λ称为矩阵A特     征值，非零向量x称为A的对应于特征值λ的特征向量。式Ax=λx也可写成( A-λE)X=0。这是n     个未知数n个方程的齐次线性方程组，它有非零解的充分必要条件是系数行列式| A-λE|=0。<br><img src="/2020/03/DLmath/6.jpg" width="50%"> </p>
<hr>
<iframe src="//player.bilibili.com/player.html?aid=6540378&bvid=BV1Ls411b7oL&cid=10640206&page=1" width="800" height="600" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<h2 id="特征值求法"><a href="#特征值求法" class="headerlink" title="特征值求法"></a>特征值求法</h2><img src="/2020/03/DLmath/7.jpg" width="80%"> 

<h2 id="相似矩阵"><a href="#相似矩阵" class="headerlink" title="相似矩阵"></a>相似矩阵</h2><p><em><em>介绍 : *</em>设A,B都是n阶矩阵，若存在可逆矩阵P，使P^(-1)AP=B，则称B是A的相似矩阵, 并称矩阵A与B相似，记为A~B。*同一线性变换,不同基下的矩阵.</em><br>对进行运算称为对进行相似变换，称可逆矩阵为相似变换矩阵。</p>
<h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>*<em>介绍 : *</em>奇异值分解在某些方面与对称矩阵或Hermite矩阵基于特征向量的对角化类似。然而这两种矩阵分解尽管有其相关性，但还是有明显的不同。谱分析的基础是对称阵特征向量的分解，而奇异值分解则是谱分析理论在任意矩阵上的推广。</p>
<ul>
<li>相似对角化 : 若A相似于一个对角矩阵.</li>
</ul>
<h2 id="施密特正交化"><a href="#施密特正交化" class="headerlink" title="施密特正交化"></a>施密特正交化</h2><img src="/2020/03/DLmath/8.jpg" width="50%"> 

<img src="/2020/03/DLmath/9.jpg" width="60%"> 

<h2 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h2><ul>
<li>L0范数 : 向量中非零元素个数</li>
<li>L1范数 : 绝对值之和,曼哈顿距离.</li>
<li>l2范数 : 欧几里得距离.</li>
<li>l正无穷 : 绝对值最大值.</li>
<li>l负无穷 : min.</li>
</ul>
<h2 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="矩阵范数"></a>矩阵范数</h2><p>矩阵范数应用 : 推荐系统正则化.</p>
]]></content>
      <categories>
        <category>学习记录</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习实践</title>
    <url>/2020/03/DLtest/</url>
    <content><![CDATA[<h1 id="MNIST数字识别"><a href="#MNIST数字识别" class="headerlink" title="MNIST数字识别"></a>MNIST数字识别</h1><ul>
<li>数据集来源<br>  git clone <a href="https://github.com/mnielsen/neural-networks-and-deep-learning.git" target="_blank" rel="noopener">https://github.com/mnielsen/neural-networks-and-deep-learning.git</a></li>
</ul>
<h2 id="MNIST数据读取"><a href="#MNIST数据读取" class="headerlink" title="MNIST数据读取"></a>MNIST数据读取</h2><ul>
<li><p>数据读取参考教程: <a href="https://blog.csdn.net/panrenlong/article/details/81736754" target="_blank" rel="noopener">https://blog.csdn.net/panrenlong/article/details/81736754</a></p>
<a id="more"></a>
</li>
<li><p>python的struct用法       </p>
<table>
<thead>
<tr>
<th>函数</th>
<th>return</th>
<th>explain</th>
</tr>
</thead>
<tbody><tr>
<td>pack(fmt,v1,v2…)</td>
<td>string</td>
<td>按照给定的格式(fmt),把数据转换成字符串(字节流),并将该字符串返回.</td>
</tr>
<tr>
<td>pack_into(fmt,buffer,offset,v1,v2…)</td>
<td>None</td>
<td>按照给定的格式(fmt),将数据转换成字符串(字节流),并将字节流写入以offset开始的buffer中.(buffer为可写的缓冲区,可用array模块)</td>
</tr>
<tr>
<td>unpack(fmt,v1,v2…..)</td>
<td>tuple</td>
<td>按照给定的格式(fmt)解析字节流,并返回解析结果</td>
</tr>
<tr>
<td>pack_from(fmt,buffer,offset)</td>
<td>tuple</td>
<td>按照给定的格式(fmt)解析以offset开始的缓冲区,并返回解析结果</td>
</tr>
<tr>
<td>calcsize(fmt)</td>
<td>size of fmt</td>
<td>计算给定的格式(fmt)占用多少字节的内存，注意对齐方式</td>
</tr>
</tbody></table>
<p>输入输出格式化</p>
<table>
<thead>
<tr>
<th>Character</th>
<th>Byte order</th>
</tr>
</thead>
<tbody><tr>
<td>@</td>
<td>4个字节</td>
</tr>
<tr>
<td>=</td>
<td>原生</td>
</tr>
<tr>
<td>&lt;</td>
<td>小端</td>
</tr>
<tr>
<td>&gt;</td>
<td>大端</td>
</tr>
<tr>
<td>!</td>
<td>网络</td>
</tr>
</tbody></table>
</li>
<li><p>具体代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">index = <span class="number">0</span></span><br><span class="line">magic, numImages , numRows , numColumns = struct.unpack_from(<span class="string">'&gt;IIII'</span>,data,index)</span><br><span class="line">index += struct.calcsize(<span class="string">'&gt;IIII'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>解决print打印bytes的话，会直接显示ascii对应的字符的问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_hex</span><span class="params">(bytes)</span>:</span></span><br><span class="line">	l = [hex(int(i)) <span class="keyword">for</span> i <span class="keyword">in</span> bytes]</span><br><span class="line">	print(<span class="string">" "</span>.join(l))<span class="comment">#Python join() 方法用于将序列中的元素以指定的字符连接生成一个新的字符串。</span></span><br></pre></td></tr></table></figure>
<p>第二种写法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">" "</span>.join([<span class="string">'%02X'</span>% i <span class="keyword">for</span> i <span class="keyword">in</span> data]))</span><br></pre></td></tr></table></figure></li>
<li><p>数据读取代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = open(<span class="string">"train-images.idx3-ubyte"</span>, <span class="string">'rb'</span>).read()</span><br><span class="line"></span><br><span class="line">index = <span class="number">0</span></span><br><span class="line">magic, numImages , numRows , numColumns = struct.unpack_from(<span class="string">'&gt;IIII'</span> ,   data , index)</span><br><span class="line">index += struct.calcsize(<span class="string">'&gt;IIII'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#img = np.array([int('%d'% i) for i in data[16:800]]).reshape(28,28)</span></span><br><span class="line"><span class="comment">#print(img) 直接读取图片</span></span><br><span class="line"></span><br><span class="line">img = struct.unpack_from(<span class="string">'784B'</span>,data,index)</span><br><span class="line">index += struct.calcsize(<span class="string">'784B'</span>);</span><br><span class="line">img = np.array(img).reshape(<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">print(img) <span class="comment"># 用struct读取图片</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure() <span class="comment">#输出一个</span></span><br><span class="line">plotwindow = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">plt.imshow(img,cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<ul>
<li>同时读取数据集与标签<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(train_path,test_path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    读取MNIST数据集</span></span><br><span class="line"><span class="string">    ``return``元祖: (数据,标签)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = open(train_path,<span class="string">'rb'</span>).read()</span><br><span class="line">    data2 = open(test_path,<span class="string">'rb'</span>).read()</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    index2 = <span class="number">8</span></span><br><span class="line">    ret_data = []</span><br><span class="line">    magic, num, width, height = struct.unpack_from(<span class="string">'&gt;IIII'</span>,data,index)</span><br><span class="line">    index += struct.calcsize(<span class="string">'&gt;IIII'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        img = struct.unpack_from(<span class="string">'%dB'</span>%(height*width),data,index)</span><br><span class="line">        lable = struct.unpack_from(<span class="string">'B'</span>,data2,index2)</span><br><span class="line">        index += struct.calcsize(<span class="string">'%dB'</span>%(height*width))</span><br><span class="line">        index2 += <span class="number">1</span></span><br><span class="line">        ret_data.append( tuple([np.array(img).reshape(width,height),lable]) )</span><br><span class="line">    <span class="keyword">return</span> ret_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = read_data(<span class="string">"MachineLearning/Digital recognition/DataSet/train-images.idx3-ubyte"</span>, \</span><br><span class="line">    <span class="string">"MachineLearning/Digital recognition/DataSet/train-labels.idx1-ubyte"</span>)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">9</span>):</span><br><span class="line">    <span class="comment">#plotwindow = fig.add_subplot(111)</span></span><br><span class="line">    plt.subplot((<span class="number">251</span> + i))</span><br><span class="line">    plt.title(<span class="string">'%d'</span>%data[i][<span class="number">1</span>])</span><br><span class="line">    plt.imshow(data[i][<span class="number">0</span>],cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h2 id="神经网络训练"><a href="#神经网络训练" class="headerlink" title="神经网络训练"></a>神经网络训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        <span class="string">"""``sizes``每一层的神经元数量(For example, if the listwas [2, 3, 1] </span></span><br><span class="line"><span class="string">        then it would be a three-layer network, with thefirst layer containing </span></span><br><span class="line"><span class="string">        2 neurons, the second layer 3 neurons,and the third layer 1 neuron) </span></span><br><span class="line"><span class="string">        ``biases``weights: (0,1)区间的随机数 """</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="string">""" ``a``输入``Return``神经网络的输出  """</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,test_data=None)</span>:</span></span><br><span class="line">        <span class="string">"""用mini-batch stochasticgradient descent训练神经网络.</span></span><br><span class="line"><span class="string">        `training_data为元祖`(x, y)表示输入和输出.  </span></span><br><span class="line"><span class="string">        `epoch表示迭代次数,.</span></span><br><span class="line"><span class="string">        `mini_batch_size`表示取样块的大小"""</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [training_data[k:k+mini_batch_size]<span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                print( <span class="string">"迭代次数 &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(j, self.evaluate(test_data), n_test) )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print( <span class="string">"迭代次数 &#123;0&#125; complete"</span>.format(j) )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        <span class="string">"""应用梯度下降更新神经网络,使用反向传播对单个mini batch.</span></span><br><span class="line"><span class="string">        ``mini_batch`` 为元祖``(x, y)``,和``eta``是learning rate."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return:``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        <span class="string">"""Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network's output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation."""</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations."""</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""sigmoid函数"""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""sigmoid函数的导数"""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<p>调用代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> network</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(train_path,test_path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    读取MNIST数据集</span></span><br><span class="line"><span class="string">    `return` tuple(数据集,标签)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = open(train_path,<span class="string">'rb'</span>).read()</span><br><span class="line">    data2 = open(test_path,<span class="string">'rb'</span>).read()</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    index2 = <span class="number">8</span></span><br><span class="line">    ret_data = []</span><br><span class="line">    magic, num, width, height = struct.unpack_from(<span class="string">'&gt;IIII'</span>,data,index)</span><br><span class="line">    index += struct.calcsize(<span class="string">'&gt;IIII'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        img = struct.unpack_from(<span class="string">'%dB'</span>%(height*width),data,index)</span><br><span class="line">        lable = struct.unpack_from(<span class="string">'B'</span>,data2,index2)</span><br><span class="line">        index += struct.calcsize(<span class="string">'%dB'</span>%(height*width))</span><br><span class="line">        index2 += <span class="number">1</span></span><br><span class="line">        ret_data.append( [np.array(img).reshape(width*height,<span class="number">1</span>),int(lable[<span class="number">0</span>])] )</span><br><span class="line">    <span class="keyword">return</span> ret_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    training_data = read_data(<span class="string">"MachineLearning/Digital recognition/DataSet/train-images.idx3-ubyte"</span>, \</span><br><span class="line">    <span class="string">"MachineLearning/Digital recognition/DataSet/train-labels.idx1-ubyte"</span>)</span><br><span class="line">    test_data = read_data(<span class="string">'MachineLearning/Digital recognition/DataSet/t10k-images.idx3-ubyte'</span>,\</span><br><span class="line">        <span class="string">"MachineLearning/Digital recognition/DataSet/t10k-labels.idx1-ubyte"</span>)</span><br><span class="line">    net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br><span class="line">    net.SGD(training_data, <span class="number">20</span>, <span class="number">10</span>, <span class="number">3.0</span>, test_data=test_data)</span><br><span class="line">    <span class="comment"># plt.figure()</span></span><br><span class="line">    <span class="comment"># for i in range(0,1):</span></span><br><span class="line">    <span class="comment">#     plt.subplot((251 + i))</span></span><br><span class="line">    <span class="comment">#     plt.title('%d'%test_data[i][1])</span></span><br><span class="line">    <span class="comment">#     plt.imshow(test_data[i][0].reshape(28,28),cmap='gray_r')</span></span><br><span class="line">    <span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络基础</title>
    <url>/2020/03/DLbasic/</url>
    <content><![CDATA[<h1 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h1><ul>
<li>来源 : 《神经网络与深度学习》(<a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener">Neuralnetworks and Deeplearning</a>)</li>
</ul>
<h2 id="常用名词"><a href="#常用名词" class="headerlink" title="常用名词"></a>常用名词</h2><ul>
<li><p>偏置 ( bias )</p>
<a id="more"></a>
</li>
<li><p>阈值 ( threshold )</p>
</li>
<li><p>梯度下降法（ gradient descent ）</p>
</li>
<li><p>平方损失（ quadratic cost ）</p>
</li>
<li><p>向量化（vectorizing）</p>
</li>
<li><p>批梯度下降 ( Batch gradient descent )</p>
</li>
<li><p>随机梯度下降 ( stochastic gradient descent )</p>
</li>
<li><p>小批的梯度下降 ( mini-batch gradient decent )<br>mini batch 下的梯度下降中降样本被分为一个个子集进行梯度下降</p>
</li>
<li><p>奇异值分解（ Singular Value Decomposition ）</p>
<h2 id="感知机规则"><a href="#感知机规则" class="headerlink" title="感知机规则"></a>感知机规则</h2><img src="/2020/03/DLbasic/1.jpg" width="60%">

</li>
</ul>
<h2 id="Sigmoid神经元"><a href="#Sigmoid神经元" class="headerlink" title="Sigmoid神经元"></a>Sigmoid神经元</h2><ul>
<li><p>S型曲线（S-Curve）多存在于分类评定模型（Logit model），逻辑回（Logistic regression）模型  </p>
<img src="/2020/03/DLbasic/2.jpg" width="40%">
</li>
<li><p>使用s曲线主要是避免阈值效应,减小微小变化对结果的影响.</p>
<img src="/2020/03/DLbasic/3.jpg" width="20%">

</li>
</ul>
<h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><ul>
<li><p>代价函数  </p>
<img src="/2020/03/DLbasic/4.jpg" width="40%">  
衡量我们当前取得的结果距离目标结果的好坏程度
数字越大说明训练效果越差
</li>
<li><p>迭代过程  </p>
<img src="/2020/03/DLbasic/5.jpg" width="30%">  
n为学习率（learning rate）

</li>
</ul>
]]></content>
      <categories>
        <category>学习记录</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown高级应用</title>
    <url>/2020/03/MarkdownNote2/</url>
    <content><![CDATA[<p>记录一些Markdown的高级用法.</p>
<a id="more"></a>
<h1 id="HTML标签"><a href="#HTML标签" class="headerlink" title="HTML标签"></a>HTML标签</h1><ul>
<li><kbd>标识</kbd> <figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span>HTML<span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><mark> 标记</mark>  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;mark&gt;标识\&lt;&#x2F;mark&gt;</span><br></pre></td></tr></table></figure></li>
<li>空格<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&amp;nbsp;&amp;emsp;&amp;ensp;</span><br></pre></td></tr></table></figure></li>
<li>居中<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;center&gt;居中&lt;&#x2F;center&gt;</span><br></pre></td></tr></table></figure></li>
<li>阅读更多<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<ul>
<li>图片<br><img src="/2020/03/MarkdownNote2/MarkdownNote2%5CIMG_6.JPG" alt><img src="/2020/03/MarkdownNote2/IMG_6.JPG" width="20%">
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![](MarkdownNote2\IMG_6.JPG)</span><br><span class="line">&lt;img src&#x3D;&quot;MarkdownNote2\IMG_6.JPG&quot; width &#x3D; &quot;20%&quot;&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h1 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h1><ul>
<li>$$ 数学公式 $$  <img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" style="border:none;">



</li>
</ul>
]]></content>
      <categories>
        <category>学习记录</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown学习记录</title>
    <url>/2020/03/MarkdownNote/</url>
    <content><![CDATA[<p>Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。Markdown 后缀为 .md; .markdown</p>
<a id="more"></a>

<h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><hr>
<h1 id="一级标题-1"><a href="#一级标题-1" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题-1"><a href="#二级标题-1" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><hr>
<h1 id="段落"><a href="#段落" class="headerlink" title="段落"></a>段落</h1><p>段落换行，段落<br>换行</p>
<hr>
<p><em>斜体</em><br><strong>加粗</strong><br><strong><em>粗斜体</em></strong><br><del>删除画线</del><br><u>下划线</u></p>
<hr>
<p>列表  </p>
<ul>
<li>1</li>
</ul>
<ul>
<li>2</li>
</ul>
<ul>
<li>3  </li>
</ul>
<p>数字 </p>
<ol>
<li>一</li>
<li>二 :<ul>
<li>嵌套</li>
<li>嵌套加4空格</li>
</ul>
</li>
</ol>
<hr>
<blockquote>
<p>第一层</p>
<blockquote>
<p>第二层</p>
<blockquote>
<p>第三层</p>
<blockquote>
<p>第四层</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<hr>
<ul>
<li>第一项  <blockquote>
<p>第一嵌套</p>
</blockquote>
</li>
</ul>
<hr>
<h1 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h1><p>生成一个脚注1[^footnote].<br>  [^footnote]: 这里是 <strong>脚注</strong> 的 <em>内容</em>.  </p>
<p>标明脚注[^1]<br>   [^1]: 脚注  </p>
<h1 id="代码插入"><a href="#代码插入" class="headerlink" title="代码插入"></a>代码插入</h1><p>代码   </p>
<pre><code>for(int i = 0;i &lt; 1;i++){

}</code></pre><p>或者   </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">i++;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt;<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p><a href="www.baidu.com">百度</a><br>&lt;<a href="http://www.baidu.com&gt;" target="_blank" rel="noopener">www.baidu.com&gt;</a></p>
<hr>
<p>链接也可以用变量来代替，文档末尾附带变量地址：<br>这个链接用 1 作为网址变量 <a href="http://www.google.com/" target="_blank" rel="noopener">Google</a><br>这个链接用 runoob 作为网址变量 <a href="http://www.baidu.com/" target="_blank" rel="noopener">Baidu</a><br>然后在文档的结尾为变量赋值（网址）</p>
<hr>
<h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h1><img src="http://TheDxr.github.io/images/note/IMG_1.jpg" size="%50">  


<p>表格   </p>
<table>
<thead>
<tr>
<th align="left">1</th>
<th align="right">2</th>
<th align="center">3</th>
</tr>
</thead>
<tbody><tr>
<td align="left">4</td>
<td align="right">5</td>
<td align="center">6</td>
</tr>
<tr>
<td align="left">7</td>
<td align="right">8</td>
<td align="center">9</td>
</tr>
</tbody></table>
<hr>
<p>常用的HTML标签<br><kbd>标签</kbd><br>转意 *\n*</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="emphasis">***</span></span><br><span class="line"><span class="section"># 段落</span></span><br><span class="line">段落换行，段落  </span><br><span class="line">换行</span><br><span class="line"><span class="emphasis">***</span></span><br><span class="line"><span class="emphasis">*斜体*</span>  </span><br><span class="line"><span class="strong">**加粗**</span>  </span><br><span class="line"><span class="strong">***粗斜体**</span>*  </span><br><span class="line">~~删除画线~~  </span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">u</span>&gt;</span></span>下划线<span class="xml"><span class="tag">&lt;/<span class="name">u</span>&gt;</span></span></span><br><span class="line"><span class="emphasis">***</span></span><br><span class="line"></span><br><span class="line">列表  </span><br><span class="line"><span class="bullet">* </span>1</span><br><span class="line"><span class="bullet">+ </span>2</span><br><span class="line"><span class="bullet">- </span>3  </span><br><span class="line"></span><br><span class="line">数字 </span><br><span class="line"><span class="bullet">1. </span>一</span><br><span class="line"><span class="bullet">2. </span>二 :</span><br><span class="line"><span class="bullet">    - </span>嵌套</span><br><span class="line"><span class="bullet">    - </span>嵌套加4空格</span><br><span class="line"></span><br><span class="line"><span class="emphasis">***</span></span><br><span class="line"><span class="quote">&gt; 第一层</span></span><br><span class="line">&gt;&gt; 第二层</span><br><span class="line">&gt;&gt;&gt; 第三层</span><br><span class="line">&gt;&gt;&gt;&gt; 第四层</span><br><span class="line"><span class="emphasis">***</span></span><br><span class="line"><span class="bullet">* </span>第一项  </span><br><span class="line"><span class="code">    &gt; 第一嵌套</span></span><br><span class="line"><span class="emphasis">***</span>  </span><br><span class="line"><span class="section"># 脚注</span></span><br><span class="line">生成一个脚注1[^footnote].</span><br><span class="line">  [^footnote]: 这里是 <span class="strong">**脚注**</span> 的 <span class="emphasis">*内容*</span>.  </span><br><span class="line"></span><br><span class="line">标明脚注[^1]   </span><br><span class="line">   [^1]: 脚注  </span><br><span class="line"></span><br><span class="line"><span class="section"># 代码插入</span></span><br><span class="line"></span><br><span class="line">代码   </span><br><span class="line"></span><br><span class="line"><span class="code">    for(int i = 0;i &lt; 1;i++)&#123;</span></span><br><span class="line"></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line">或者   </span><br><span class="line"></span><br><span class="line"><span class="emphasis">***</span></span><br><span class="line"><span class="section"># 链接  </span></span><br><span class="line">[<span class="string">百度</span>](<span class="link">www.baidu.com</span>)  </span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">www.baidu.com</span>&gt;</span></span></span><br><span class="line"><span class="emphasis">***</span></span><br><span class="line">链接也可以用变量来代替，文档末尾附带变量地址：</span><br><span class="line">这个链接用 1 作为网址变量 [<span class="string">Google</span>][<span class="symbol">1</span>]</span><br><span class="line">这个链接用 runoob 作为网址变量 [<span class="string">Baidu</span>][<span class="symbol">2</span>]</span><br><span class="line">然后在文档的结尾为变量赋值（网址）</span><br><span class="line"></span><br><span class="line">  [1]: http://www.google.com/</span><br><span class="line">  [2]: http://www.baidu.com/</span><br><span class="line"><span class="emphasis">***</span></span><br><span class="line"><span class="section"># 图片</span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"https://github.com/TheDxr/TheDxr.github.io/blob/master/images/note/IMG_1.jpg"</span> <span class="attr">size</span> = <span class="string">"%50"</span>&gt;</span></span>  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">表格   </span><br><span class="line"></span><br><span class="line">|  1  |  2  |  3  |</span><br><span class="line">| :--- | ---: | :---: |</span><br><span class="line">|  4  |  5  |  6  |</span><br><span class="line">|  7  |  8  |  9  |   </span><br><span class="line"></span><br><span class="line"><span class="emphasis">***</span></span><br><span class="line">常用的HTML标签  </span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span></span>标签<span class="xml"><span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span>  </span><br><span class="line">转意 \<span class="emphasis">*\n\*</span></span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>学习记录</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/03/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<a id="more"></a>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>其他记录</category>
      </categories>
  </entry>
</search>
